{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT1043 A2 Assignment - Derek Goh Kai Shen (33521247)\n",
    "\n",
    "## Part A: Classification\n",
    "\n",
    "### A1. Supervised Learning\n",
    "### 1. Definition of supervised machine learning, the notion of labelled data, and train and test datasets.\n",
    "- Supervised machine learning is a subset of machine learning where the model is trained upon a labelled dataset to yield a desired output that we can predict. Some of the common algorithms used to train models are neural networks, naive bayes, linear regression, logistic regression, support vector machines(SVM) and more. \n",
    "\n",
    "- All the data used in training the model is labelled, as in referring to data that has been classified with the correct output. \n",
    "\n",
    "- The training dataset is a set of data that is correctly labelled and includes the input and the respective correct output, which allows the model to learn the relationship between the input and output. The model is then tested on a separate dataset, known as the test dataset, to evaluate its loss function and accuracy index. The model is then tweaked to minimise the loss function and improve the accuracy index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# keras = tf.keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Input\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras import regularizers\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Loading dataset \n",
    "# data = pd.read_csv('FIT1043-MusicGenre-Dataset.csv')\n",
    "# data = data.dropna()\n",
    "\n",
    "# # Scaling and normalizing the data\n",
    "# df = data.copy()\n",
    "\n",
    "\n",
    "# tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, max_features=5000, stop_words='english')\n",
    "# artist_name_vectorized = tfidf.fit_transform(df['artist_name'])\n",
    "\n",
    "# # Keep the matrix sparse, don't convert to dense with toarray()\n",
    "# df_artist = pd.DataFrame.sparse.from_spmatrix(artist_name_vectorized, columns=[f'artist_{i}' for i in range(artist_name_vectorized.shape[1])])\n",
    "\n",
    "# tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, max_features=5000, stop_words='english')\n",
    "# track_name_vectorized = tfidf.fit_transform(df['track_name'])\n",
    "\n",
    "# # Keep the matrix sparse, don't convert to dense with toarray()\n",
    "# df_track = pd.DataFrame.sparse.from_spmatrix(track_name_vectorized, columns=[f'track_{i}' for i in range(track_name_vectorized.shape[1])])\n",
    "\n",
    "# df = df.drop(columns=['artist_name', 'track_name'])\n",
    "# df = pd.concat([df, df_artist, df_track], axis=1)\n",
    "\n",
    "# label = df['music_genre']\n",
    "# features = df.drop(columns=['music_genre', 'instance_id'])\n",
    "\n",
    "# sclr = StandardScaler()\n",
    "# features = pd.DataFrame(sclr.fit_transform(features), columns=features.columns)\n",
    "\n",
    "# features_train, features_test, label_train, label_test = train_test_split(features, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Building the model\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Input(shape=(features_train.shape[1],)))\n",
    "# model.add(Dense(13, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "# model.add(Dense(20, activation='relu', kernel_regularizer= regularizers.l2(0.005)))\n",
    "\n",
    "# model.add(Dense(20, activation='relu', kernel_regularizer= regularizers.l2(0.01)))\n",
    "\n",
    "# # Output layer\n",
    "# model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# # Compiling the model\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Define the early stopping criteria\n",
    "# stop_early = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "\n",
    "# # Training the model\n",
    "# model.fit(features_train, label_train, epochs=100, batch_size=32, validation_data=(features_test, label_test), callbacks=[stop_early])\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss, accuracy = model.evaluate(features_test, label_test)\n",
    "# print(f'Accuracy: {accuracy*100}%')\n",
    "\n",
    "# model.save(\"1000_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, TextVectorization, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Loading dataset \n",
    "data = pd.read_csv('FIT1043-MusicGenre-Dataset.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "# Scaling and normalizing the data\n",
    "df = data.copy()\n",
    "\n",
    "# vectorise text data into int\n",
    "artist_name_vectorizer = TextVectorization(output_mode='int')\n",
    "artist_name_vectorizer.adapt(df['artist_name'])\n",
    "artist_name_vectorized = artist_name_vectorizer(df['artist_name'])\n",
    "\n",
    "# flatten\n",
    "artist_name_vectorized = tf.reduce_mean(artist_name_vectorized, axis=-1)\n",
    "\n",
    "df['artist_name'] = artist_name_vectorized.numpy()\n",
    "\n",
    "track_name_vectorizer = TextVectorization(output_mode='int')\n",
    "track_name_vectorizer.adapt(df['track_name'])\n",
    "track_name_vectorized = track_name_vectorizer(df['track_name'])\n",
    "\n",
    "# flatten\n",
    "track_name_vectorized = tf.reduce_mean(track_name_vectorized, axis=-1)\n",
    "\n",
    "df['track_name'] = track_name_vectorized.numpy()\n",
    "\n",
    "# Seperating features and the label\n",
    "features = df.drop(columns=['music_genre', 'instance_id'])\n",
    "label = df['music_genre']\n",
    "\n",
    "# Normalize\n",
    "sclr = StandardScaler()\n",
    "features = pd.DataFrame(sclr.fit_transform(features), columns=features.columns)\n",
    "\n",
    "features_train, features_test, label_train, label_test = train_test_split(features, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Building the model\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Input(shape=(features_train.shape[1],)))\n",
    "# model.add(Dense(13, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "# model.add(Dense(20, activation='relu', kernel_regularizer= regularizers.l2(0.005)))\n",
    "\n",
    "# model.add(Dense(20, activation='relu', kernel_regularizer= regularizers.l2(0.01)))\n",
    "\n",
    "# # Output layer\n",
    "# model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# # Compiling the model\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# # Define the early stopping criteria\n",
    "# stop_early = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "\n",
    "# # Training the model\n",
    "# model.fit(features_train, label_train, epochs=100, batch_size=64, validation_data=(features_test, label_test), callbacks=[stop_early])\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss, accuracy = model.evaluate(features_test, label_test)\n",
    "# print(f'Accuracy: {accuracy*100}%')\n",
    "\n",
    "# model.save(\"model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Classification (Training)\n",
    "### 1. Differences between binary and multi-class classification.\n",
    "\n",
    "- Binary classification is a type of classification where the model is trained to predict between two classes, such as true or false, spam or no, 0 or 1, and so on. The output is a boolean, which is either True or False.\n",
    "\n",
    "- Multi-class classification is a type of classification where the model is trained to predict between multiple classes, such as classifying between genre of music, colour schemes, dog breeds, and so on. The output is a class label, which is one of the classes that the model is trained to predict.\n",
    "\n",
    "### 2. Normalising/Scaling Data for Preparation for Classification\n",
    "\n",
    "- Normalsing or Scaling of data is important as it allows the model gradient descent to converge faster, as all the features are on the same scale. This is important as we want to scale the data when we are using algorithms using distance between data points, such as Support Vector Machines (SVM) and K-Nearest Neighbours (KNN). For example, if we have a dataset with features that have totally different scales, such as age and income, the model will be biased towards the feature with larger scale, which will be the income in this case.\n",
    "\n",
    "- There are many ways to scale the data, such as Min-Max Scaling, Standard Scaling, Robust Scaling, and Normalisation. Min-Max Scaling scales the data to a range between 0 and 1, Standard Scaling scales the data to have a mean of 0 and a standard deviation of 1, Robust Scaling scales the data to the interquartile range, and Normalisation scales the data to have a magnitude of 1. The best scaling method for predicting the genre of the music is Standard Scaling, as it scales the data to have a mean of 0 and a standard deviation of 1, which is important for algorithms that use distance between data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# sc = StandardScaler()\n",
    "\n",
    "# # Normalising training and testing data\n",
    "# features_train = sc.fit_transform(features_train)\n",
    "# features_test = sc.transform(features_test)\n",
    "\n",
    "\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# bag_clf = BaggingClassifier(\n",
    "#     DecisionTreeClassifier(random_state=42), \n",
    "#     n_estimators=500,\n",
    "#     max_samples=100, \n",
    "#     bootstrap=True, \n",
    "#     n_jobs=-1, \n",
    "#     random_state=42)\n",
    "\n",
    "# bag_clf.fit(features_train, label_train)\n",
    "\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# ada_clf = AdaBoostClassifier(\n",
    "#     DecisionTreeClassifier(max_depth=1), \n",
    "#     n_estimators=200,\n",
    "#     algorithm=\"SAMME.R\", \n",
    "#     learning_rate=0.5, \n",
    "#     random_state=42)\n",
    "\n",
    "# ada_clf.fit(features_train, label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using Support Vector Machines (SVM) for Classification\n",
    "\n",
    "- SVM is a supervised learning algorithm used in machine learning to solve classification problems. It's very good in solving binary classification problems, but can also be used for multi-class classification problems. The algorithm works by seperating the classes with a hyperplane that has the maximum distance between the nearest data points of the classes, which can be referred to as the margin. The data points that are closest to that hyperplane are called support vectors. The hyperplane can be linear or non-linear, depending on the kernel used. The most common kernel used is the Radial Basis Function (RBF) kernel, or the Gaussian kernel. RBF kernel, which is a non-linear kernel, is used when the data is not linearly separable, and the linear kernel is used when the data is linearly separable.\n",
    "\n",
    "- Since SVM are fundamentally binary classifiers, to allow them to support multi-class classifications, we can employ either One-Vs-Rest (OvR) or One-Vs-One(OvO) strategies. OvR trains a binary classifier for each class, which is then used to predict the class with the highest confidence score. OvO trains a binary classifier for each pair of classes, which is then used to predict the class with the most votes. OvR is more efficient than OvO, as it requires less training time, but OvO is more accurate than OvR, as it requires more training time. Thus, we have to balance between efficiency and accuracy when choosing between OvR and OvO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[190   3  11   0 101  25  59  31  19 105]\n",
      " [ 16 349  29  39  10  21   0   9   0   4]\n",
      " [ 23  33 306   4  70  31   2  56   0  22]\n",
      " [  9  18  11 425   4  11   0  16   0   1]\n",
      " [ 35  14  36   0 296  15  13  33   5  83]\n",
      " [ 43  22  31   7  33 311  20  40  10  13]\n",
      " [ 21   0   0   3  11   5 309   2 155  36]\n",
      " [ 15   7  58  37  34  61  12 240   1  20]\n",
      " [ 23   0   2   0  11   3 239   1 169  59]\n",
      " [ 58   2   5   0  40   1  10  15  24 380]]\n",
      "Accuracy: 0.5729969183359014\n"
     ]
    }
   ],
   "source": [
    "# Building SVM model to classify the music genre.\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel= 'rbf', random_state=42)\n",
    "classifier.fit(features_train, label_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "label_pred = classifier.predict(features_test)\n",
    "\n",
    "# Making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(label_test, label_pred)\n",
    "print(f'Confusion Matrix: \\n{cm}')\n",
    "\n",
    "accuracy = accuracy_score(label_test, label_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[237   7  13   1  66  18  33  31  37 101]\n",
      " [ 12 384  25  14  10  23   0   5   2   2]\n",
      " [ 27  23 333   4  47  29   3  57   2  22]\n",
      " [ 13   9  14 431   2   3   0  21   0   2]\n",
      " [ 35   5  24   0 338  12   5  25  11  75]\n",
      " [ 38  12  34   5  17 334   7  55   9  19]\n",
      " [ 22   1   1   0   3   5 240   5 238  27]\n",
      " [ 19   1  54  23  23  49   6 290   4  16]\n",
      " [ 18   1   1   0   3   1 257   2 176  48]\n",
      " [ 56   7   8   0  51   3  11  14  33 352]]\n",
      "Accuracy: 0.5999614791987673\n"
     ]
    }
   ],
   "source": [
    "# Using XGBoost to classify the music genre\n",
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(features_train, label_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "label_pred = classifier.predict(features_test)\n",
    "\n",
    "# Making the confusion matrix\n",
    "cm = confusion_matrix(label_test, label_pred)\n",
    "print(f'Confusion Matrix: \\n{cm}')\n",
    "\n",
    "accuracy = accuracy_score(label_test, label_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# saving the model\n",
    "classifier.save_model('xgb_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[182   8  17   0  79  30  48  35  35 110]\n",
      " [ 21 361  21  36   9  23   0   4   0   2]\n",
      " [ 36  40 293   7  55  35   3  55   2  21]\n",
      " [ 12  17  11 424   2   9   0  19   0   1]\n",
      " [ 58  13  37   1 288  11  12  25  12  73]\n",
      " [ 49  29  46  10  27 270  13  59  10  17]\n",
      " [ 32   2   2   0   6   3 262   2 208  25]\n",
      " [ 35  11  72  29  32  65  12 211   4  14]\n",
      " [ 37   1   2   0  10   3 267   3 147  37]\n",
      " [ 91   5   9   1  69   7  25  15  28 285]]\n",
      "Accuracy: 0.5244607087827426\n"
     ]
    }
   ],
   "source": [
    "# Using Random Forest to classify the music genre\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=42)\n",
    "classifier.fit(features_train, label_train)\n",
    "\n",
    "label_pred = classifier.predict(features_test)\n",
    "\n",
    "cm = confusion_matrix(label_test, label_pred)\n",
    "print(f'Confusion Matrix: \\n{cm}')\n",
    "\n",
    "accuracy = accuracy_score(label_test, label_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[168  17  36   8  73  48  39  29  47  79]\n",
      " [ 12 319  43  34  12  39   0  13   0   5]\n",
      " [ 35  40 234  18  66  47   1  85   5  16]\n",
      " [ 13  40   9 381   5   9   0  35   0   3]\n",
      " [ 67  16  57   7 232  25  10  43  13  60]\n",
      " [ 53  34  38  14  22 255  12  70   8  24]\n",
      " [ 43   0   5   0  12  14 210  16 224  18]\n",
      " [ 43  12  57  36  39  63   8 200   6  21]\n",
      " [ 32   1   4   0  25   5 258   2 127  53]\n",
      " [ 80   8  34   4  68  11  35  18  45 232]]\n",
      "Accuracy: 0.45416024653312786\n"
     ]
    }
   ],
   "source": [
    "# Using Decision Tree to classify the music genre\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "classifier.fit(features_train, label_train)\n",
    "\n",
    "label_pred = classifier.predict(features_test)\n",
    "\n",
    "cm = confusion_matrix(label_test, label_pred)\n",
    "print(f'Confusion Matrix: \\n{cm}')\n",
    "\n",
    "accuracy = accuracy_score(label_test, label_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
