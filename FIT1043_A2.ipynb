{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT1043 A2 Assignment - Derek Goh Kai Shen (33521247)\n",
    "\n",
    "## Part A: Classification\n",
    "\n",
    "### A1. Supervised Learning\n",
    "### 1. Definition of supervised machine learning, the notion of labelled data, and train and test datasets.\n",
    "- Supervised machine learning is a subset of machine learning where the model is trained upon a labelled dataset to yield a desired output that we can predict. Some of the common algorithms used to train models are neural networks, naive bayes, linear regression, logistic regression, support vector machines(SVM) and more. \n",
    "\n",
    "- All the data used in training the model is labelled, as in referring to data that has been classified with the correct output. \n",
    "\n",
    "- The training dataset is a set of data that is correctly labelled and includes the input and the respective correct output, which allows the model to learn the relationship between the input and output. The model is then tested on a separate dataset, known as the test dataset, to evaluate its loss function and accuracy index. The model is then tweaked to minimise the loss function and improve the accuracy index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: instance_id\n",
      "Mean: 55769.94125125202\n",
      "Median: 55740.5\n",
      "Variance: 431077085.5881422\n",
      "\n",
      "Columns: artist_name\n",
      "Mean: 232.45172971723554\n",
      "Median: 140.0\n",
      "Variance: 70000.838348705\n",
      "\n",
      "Columns: popularity\n",
      "Mean: 0.44614688373837774\n",
      "Median: 0.4545454545454546\n",
      "Variance: 0.024664485986313586\n",
      "\n",
      "Columns: acousticness\n",
      "Mean: 0.3090943232240156\n",
      "Median: 0.1465863453815261\n",
      "Variance: 0.11796946241528146\n",
      "\n",
      "Columns: danceability\n",
      "Mean: 0.5379725391062831\n",
      "Median: 0.5498704663212434\n",
      "Variance: 0.03726771827963333\n",
      "\n",
      "Columns: duration_ms\n",
      "Mean: 0.05113029037247991\n",
      "Median: 0.047121970833245944\n",
      "Variance: 0.0006089345985379015\n",
      "\n",
      "Columns: energy\n",
      "Mean: 0.5998788631909636\n",
      "Median: 0.6453644931717639\n",
      "Variance: 0.07072631712772043\n",
      "\n",
      "Columns: instrumentalness\n",
      "Mean: 0.18368853299591772\n",
      "Median: 0.0001676706827309237\n",
      "Variance: 0.10759812421754344\n",
      "\n",
      "Columns: liveness\n",
      "Mean: 0.18610160615273116\n",
      "Median: 0.11746589520664827\n",
      "Variance: 0.026772982022279977\n",
      "\n",
      "Columns: loudness\n",
      "Mean: 0.7457901135553896\n",
      "Median: 0.7826934435912581\n",
      "Variance: 0.014808979811320028\n",
      "\n",
      "Columns: speechiness\n",
      "Mean: 0.07802340819320858\n",
      "Median: 0.028705012504077414\n",
      "Variance: 0.01253463104569376\n",
      "\n",
      "Columns: tempo\n",
      "Mean: 0.4606926418008798\n",
      "Median: 0.4598610222181586\n",
      "Variance: 0.027107558500131534\n",
      "\n",
      "Columns: valence\n",
      "Mean: 0.4636582264685016\n",
      "Median: 0.4560161779575329\n",
      "Variance: 0.06282633087882414\n",
      "\n",
      "Columns: music_genre\n",
      "Mean: 4.480083211341397\n",
      "Median: 4.0\n",
      "Variance: 8.237747159543225\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.1191 - loss: 27.8987 - val_accuracy: 0.1448 - val_loss: 2.3799\n",
      "Epoch 2/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.1605 - loss: 2.3384 - val_accuracy: 0.1705 - val_loss: 2.2894\n",
      "Epoch 3/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.1845 - loss: 2.2716 - val_accuracy: 0.2146 - val_loss: 2.1595\n",
      "Epoch 4/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.2139 - loss: 2.1922 - val_accuracy: 0.1878 - val_loss: 2.2210\n",
      "Epoch 5/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.2231 - loss: 2.1738 - val_accuracy: 0.2373 - val_loss: 2.1540\n",
      "Epoch 6/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.2362 - loss: 2.1158 - val_accuracy: 0.2533 - val_loss: 2.0794\n",
      "Epoch 7/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.2516 - loss: 2.0666 - val_accuracy: 0.2490 - val_loss: 2.0586\n",
      "Epoch 8/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.2490 - loss: 2.0703 - val_accuracy: 0.2612 - val_loss: 1.9988\n",
      "Epoch 9/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.2670 - loss: 2.0037 - val_accuracy: 0.2435 - val_loss: 2.0193\n",
      "Epoch 10/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.2707 - loss: 1.9949 - val_accuracy: 0.2845 - val_loss: 1.9579\n",
      "Epoch 11/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.2749 - loss: 1.9744 - val_accuracy: 0.2775 - val_loss: 1.9537\n",
      "Epoch 12/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.2741 - loss: 1.9665 - val_accuracy: 0.3051 - val_loss: 1.9284\n",
      "Epoch 13/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.2842 - loss: 1.9675 - val_accuracy: 0.2822 - val_loss: 1.9623\n",
      "Epoch 14/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.2802 - loss: 1.9452 - val_accuracy: 0.2953 - val_loss: 1.8999\n",
      "Epoch 15/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.2962 - loss: 1.9043 - val_accuracy: 0.2818 - val_loss: 1.9203\n",
      "Epoch 16/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3069 - loss: 1.9025 - val_accuracy: 0.2752 - val_loss: 1.9488\n",
      "Epoch 17/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.2946 - loss: 1.9200 - val_accuracy: 0.2933 - val_loss: 1.8969\n",
      "Epoch 18/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.2931 - loss: 1.9260 - val_accuracy: 0.2623 - val_loss: 2.0323\n",
      "Epoch 19/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3032 - loss: 1.9041 - val_accuracy: 0.2812 - val_loss: 1.9917\n",
      "Epoch 20/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3112 - loss: 1.8866 - val_accuracy: 0.3047 - val_loss: 1.8628\n",
      "Epoch 21/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3026 - loss: 1.8938 - val_accuracy: 0.2985 - val_loss: 1.8553\n",
      "Epoch 22/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3030 - loss: 1.8917 - val_accuracy: 0.2866 - val_loss: 1.9519\n",
      "Epoch 23/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3121 - loss: 1.8772 - val_accuracy: 0.3016 - val_loss: 1.8900\n",
      "Epoch 24/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3213 - loss: 1.8595 - val_accuracy: 0.3261 - val_loss: 1.8384\n",
      "Epoch 25/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3195 - loss: 1.8717 - val_accuracy: 0.3226 - val_loss: 1.8448\n",
      "Epoch 26/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3067 - loss: 1.9116 - val_accuracy: 0.2866 - val_loss: 1.9364\n",
      "Epoch 27/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3361 - loss: 1.8479 - val_accuracy: 0.3143 - val_loss: 1.8457\n",
      "Epoch 28/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3221 - loss: 1.8701 - val_accuracy: 0.3461 - val_loss: 1.8091\n",
      "Epoch 29/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3342 - loss: 1.8401 - val_accuracy: 0.3369 - val_loss: 1.8176\n",
      "Epoch 30/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3384 - loss: 1.8383 - val_accuracy: 0.3507 - val_loss: 1.7888\n",
      "Epoch 31/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3412 - loss: 1.8248 - val_accuracy: 0.3398 - val_loss: 1.7988\n",
      "Epoch 32/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3441 - loss: 1.8300 - val_accuracy: 0.3523 - val_loss: 1.7944\n",
      "Epoch 33/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3530 - loss: 1.7974 - val_accuracy: 0.3658 - val_loss: 1.7595\n",
      "Epoch 34/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3482 - loss: 1.8340 - val_accuracy: 0.3590 - val_loss: 1.7548\n",
      "Epoch 35/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3644 - loss: 1.7791 - val_accuracy: 0.3648 - val_loss: 1.7524\n",
      "Epoch 36/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3569 - loss: 1.7792 - val_accuracy: 0.3415 - val_loss: 1.8526\n",
      "Epoch 37/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3626 - loss: 1.7953 - val_accuracy: 0.3629 - val_loss: 1.7612\n",
      "Epoch 38/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3582 - loss: 1.8052 - val_accuracy: 0.3665 - val_loss: 1.7587\n",
      "Epoch 39/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.3694 - loss: 1.7605 - val_accuracy: 0.3342 - val_loss: 1.8665\n",
      "Epoch 40/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.3763 - loss: 1.7738 - val_accuracy: 0.3062 - val_loss: 2.0116\n",
      "Epoch 41/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3613 - loss: 1.8019 - val_accuracy: 0.3388 - val_loss: 1.7969\n",
      "Epoch 42/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3737 - loss: 1.7690 - val_accuracy: 0.3783 - val_loss: 1.7420\n",
      "Epoch 43/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3662 - loss: 1.7913 - val_accuracy: 0.3384 - val_loss: 1.9661\n",
      "Epoch 44/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.3751 - loss: 1.7594 - val_accuracy: 0.3659 - val_loss: 1.7666\n",
      "Epoch 45/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3710 - loss: 1.7566 - val_accuracy: 0.3767 - val_loss: 1.7368\n",
      "Epoch 46/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.3721 - loss: 1.7683 - val_accuracy: 0.3771 - val_loss: 1.7198\n",
      "Epoch 47/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.3712 - loss: 1.7782 - val_accuracy: 0.3357 - val_loss: 1.8447\n",
      "Epoch 48/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3763 - loss: 1.7479 - val_accuracy: 0.3713 - val_loss: 1.7491\n",
      "Epoch 49/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3685 - loss: 1.7691 - val_accuracy: 0.3877 - val_loss: 1.7156\n",
      "Epoch 50/50\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.3869 - loss: 1.7403 - val_accuracy: 0.3713 - val_loss: 1.7543\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3622 - loss: 1.7570\n",
      "Accuracy: 37.13405132293701%\n"
     ]
    }
   ],
   "source": [
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, TextVectorization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import platform\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Loading dataset \n",
    "data = pd.read_csv('FIT1043-MusicGenre-Dataset.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "# for column in data.select_dtypes(include=['float64', 'int64']).columns:\n",
    "#     print(f'Columns: {column}')\n",
    "#     print(f'Mean: {data[column].mean()}')\n",
    "#     print(f'Median: {data[column].median()}')\n",
    "#     print(f'Variance: {data[column].var()}\\n')\n",
    "\n",
    "# Scaling and normalizing the data\n",
    "df_scaled = data.copy()\n",
    "\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "# Normalizing columns\n",
    "col_norm = ['popularity', 'duration_ms', 'loudness', 'tempo', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'speechiness', 'valence']\n",
    "for col in col_norm:\n",
    "    df_scaled[col] = minmax.fit_transform(df_scaled[col].values.reshape(-1, 1))\n",
    "\n",
    "# vectorise text data into int\n",
    "artist_name_vectorizer = TextVectorization(output_mode='int')\n",
    "artist_name_vectorizer.adapt(df_scaled['artist_name'])\n",
    "artist_name_vectorized = artist_name_vectorizer(df_scaled['artist_name'])\n",
    "\n",
    "# flatten\n",
    "artist_name_vectorized = tf.reduce_mean(artist_name_vectorized, axis=-1)\n",
    "\n",
    "df_scaled['artist_name'] = artist_name_vectorized.numpy()\n",
    "\n",
    "\n",
    "#debug\n",
    "for column in df_scaled.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    print(f'Columns: {column}')\n",
    "    print(f'Mean: {df_scaled[column].mean()}')\n",
    "    print(f'Median: {df_scaled[column].median()}')\n",
    "    print(f'Variance: {df_scaled[column].var()}\\n')\n",
    "\n",
    "# Seperating features and the label\n",
    "features = df_scaled.select_dtypes(include=[np.number])\n",
    "features = features.drop(columns=['music_genre']).drop(columns=['instance_id']).drop(columns=['duration_ms'])\n",
    "label = data.iloc[:, -1]\n",
    "\n",
    "features_train, features_test, label_train, label_test = train_test_split(features, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Building the model\n",
    "input_size = features_train.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(input_size, input_dim=input_size, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='relu', kernel_regularizer= regularizers.l2(0.01)))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the early stopping criteria\n",
    "stop_early = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "# Training the model\n",
    "model.fit(features_train, label_train, epochs=50, batch_size=64, validation_data=(features_test, label_test), callbacks=[stop_early])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(features_test, label_test)\n",
    "print(f'Accuracy: {accuracy*100}%')\n",
    "\n",
    "#debug\n",
    "# print(features_train.shape)\n",
    "# print(features_test.shape)\n",
    "# print(label_train.shape)\n",
    "# print(label_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Classification (Training)\n",
    "### 1. Differences between binary and multi-class classification.\n",
    "\n",
    "- Binary classification is a type of classification where the model is trained to predict between two classes, such as true or false, spam or no, 0 or 1, and so on. The output is a boolean, which is either True or False.\n",
    "\n",
    "- Multi-class classification is a type of classification where the model is trained to predict between multiple classes, such as classifying between genre of music, colour schemes, dog breeds, and so on. The output is a class label, which is one of the classes that the model is trained to predict.\n",
    "\n",
    "### 2. Normalising/Scaling Data for Preparation for Classification\n",
    "\n",
    "- Normalsing or Scaling of data is important as it allows the model gradient descent to converge faster, as all the features are on the same scale. This is important as we want to scale the data when we are using algorithms using distance between data points, such as Support Vector Machines (SVM) and K-Nearest Neighbours (KNN). For example, if we have a dataset with features that have totally different scales, such as age and income, the model will be biased towards the feature with larger scale, which will be the income in this case.\n",
    "\n",
    "- There are many ways to scale the data, such as Min-Max Scaling, Standard Scaling, Robust Scaling, and Normalisation. Min-Max Scaling scales the data to a range between 0 and 1, Standard Scaling scales the data to have a mean of 0 and a standard deviation of 1, Robust Scaling scales the data to the interquartile range, and Normalisation scales the data to have a magnitude of 1. The best scaling method for predicting the genre of the music is Standard Scaling, as it scales the data to have a mean of 0 and a standard deviation of 1, which is important for algorithms that use distance between data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# sc = StandardScaler()\n",
    "\n",
    "# # Normalising training and testing data\n",
    "# features_train = sc.fit_transform(features_train)\n",
    "# features_test = sc.transform(features_test)\n",
    "\n",
    "\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# bag_clf = BaggingClassifier(\n",
    "#     DecisionTreeClassifier(random_state=42), \n",
    "#     n_estimators=500,\n",
    "#     max_samples=100, \n",
    "#     bootstrap=True, \n",
    "#     n_jobs=-1, \n",
    "#     random_state=42)\n",
    "\n",
    "# bag_clf.fit(features_train, label_train)\n",
    "\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# ada_clf = AdaBoostClassifier(\n",
    "#     DecisionTreeClassifier(max_depth=1), \n",
    "#     n_estimators=200,\n",
    "#     algorithm=\"SAMME.R\", \n",
    "#     learning_rate=0.5, \n",
    "#     random_state=42)\n",
    "\n",
    "# ada_clf.fit(features_train, label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using Support Vector Machines (SVM) for Classification\n",
    "\n",
    "- SVM is a supervised learning algorithm used in machine learning to solve classification problems. It's very good in solving binary classification problems, but can also be used for multi-class classification problems. The algorithm works by seperating the classes with a hyperplane that has the maximum distance between the nearest data points of the classes, which can be referred to as the margin. The data points that are closest to that hyperplane are called support vectors. The hyperplane can be linear or non-linear, depending on the kernel used. The most common kernel used is the Radial Basis Function (RBF) kernel, or the Gaussian kernel. RBF kernel, which is a non-linear kernel, is used when the data is not linearly separable, and the linear kernel is used when the data is linearly separable.\n",
    "\n",
    "- Since SVM are fundamentally binary classifiers, to allow them to support multi-class classifications, we can employ either One-Vs-Rest (OvR) or One-Vs-One(OvO) strategies. OvR trains a binary classifier for each class, which is then used to predict the class with the highest confidence score. OvO trains a binary classifier for each pair of classes, which is then used to predict the class with the most votes. OvR is more efficient than OvO, as it requires less training time, but OvO is more accurate than OvR, as it requires more training time. Thus, we have to balance between efficiency and accuracy when choosing between OvR and OvO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[184   0   9   0 116  29  61  31  14 100]\n",
      " [ 20 312  39  54  20  18   0  10   0   4]\n",
      " [ 22  46 283   4  81  32   1  53   0  25]\n",
      " [ 12  15  11 426   4  11   0  15   0   1]\n",
      " [ 30  14  47   0 272  10  17  49   4  87]\n",
      " [ 42  22  32   6  35 300  19  51   7  16]\n",
      " [ 19   0   2   0  17   6 322   1 140  35]\n",
      " [ 16   9  75  35  38  64  12 213   1  22]\n",
      " [ 27   0   2   0  14   3 249   0 152  60]\n",
      " [ 61   2   2   0  49   5  12  19  26 359]]\n",
      "Accuracy: 0.5437211093990755\n"
     ]
    }
   ],
   "source": [
    "# Building SVM model to classify the music genre.\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel= 'rbf', random_state=42)\n",
    "classifier.fit(features_train, label_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "label_pred = classifier.predict(features_test)\n",
    "\n",
    "# Making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(label_test, label_pred)\n",
    "print(f'Confusion Matrix: \\n{cm}')\n",
    "\n",
    "accuracy = accuracy_score(label_test, label_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[201   6  15   3  75  21  48  32  35 108]\n",
      " [ 15 368  31  22   9  17   0  10   0   5]\n",
      " [ 30  22 305   5  63  33   1  65   0  23]\n",
      " [ 13  12  16 423   4   8   0  17   0   2]\n",
      " [ 40   5  28   0 314  11   8  29   7  88]\n",
      " [ 34  21  31   7  18 319  14  61   9  16]\n",
      " [ 22   0   2   0   5   6 233   3 245  26]\n",
      " [ 19   2  58  27  28  59   9 259   1  23]\n",
      " [ 21   1   1   0   2   2 257   4 169  50]\n",
      " [ 55   6   8   0  52   3  14   9  31 357]]\n",
      "Accuracy: 0.5677966101694916\n"
     ]
    }
   ],
   "source": [
    "# Using XGBoost to classify the music genre\n",
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(features_train, label_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "label_pred = classifier.predict(features_test)\n",
    "\n",
    "# Making the confusion matrix\n",
    "cm = confusion_matrix(label_test, label_pred)\n",
    "print(f'Confusion Matrix: \\n{cm}')\n",
    "\n",
    "accuracy = accuracy_score(label_test, label_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[205   6  21   0  86  17  49  22  35 103]\n",
      " [ 17 351  32  33  11  22   1   5   1   4]\n",
      " [ 38  56 284   7  46  36   3  57   0  20]\n",
      " [ 12  21  17 414   5   9   0  13   0   4]\n",
      " [ 64  15  39   0 281   6  12  27  11  75]\n",
      " [ 51  28  37   7  17 286  11  60  16  17]\n",
      " [ 27   1   2   0   7   5 247   3 231  19]\n",
      " [ 30  12  76  38  32  55  14 204   3  21]\n",
      " [ 31   1   1   0  11   2 268   4 139  50]\n",
      " [ 89   5  16   1  75   4  22  17  31 275]]\n",
      "Accuracy: 0.5173343605546995\n"
     ]
    }
   ],
   "source": [
    "# Using Random Forest to classify the music genre\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=42)\n",
    "classifier.fit(features_train, label_train)\n",
    "\n",
    "label_pred = classifier.predict(features_test)\n",
    "\n",
    "cm = confusion_matrix(label_test, label_pred)\n",
    "print(f'Confusion Matrix: \\n{cm}')\n",
    "\n",
    "accuracy = accuracy_score(label_test, label_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[156  14  31   8  80  44  42  42  48  79]\n",
      " [ 15 323  43  32  12  31   1  13   0   7]\n",
      " [ 23  55 219  17  60  60   6  85   2  20]\n",
      " [ 12  43  20 375   2  10   0  26   1   6]\n",
      " [ 64  16  49   3 225  24  13  54  11  71]\n",
      " [ 50  40  49  12  27 244   9  68  10  21]\n",
      " [ 35   0   6   0  12  12 214  15 219  29]\n",
      " [ 31  16  70  41  40  69   9 182   9  18]\n",
      " [ 50   1   3   0  15   6 243   7 138  44]\n",
      " [ 89   8  30   2  78  14  31  22  37 224]]\n",
      "Accuracy: 0.44298921417565484\n"
     ]
    }
   ],
   "source": [
    "# Using Decision Tree to classify the music genre\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "classifier.fit(features_train, label_train)\n",
    "\n",
    "label_pred = classifier.predict(features_test)\n",
    "\n",
    "cm = confusion_matrix(label_test, label_pred)\n",
    "print(f'Confusion Matrix: \\n{cm}')\n",
    "\n",
    "accuracy = accuracy_score(label_test, label_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
